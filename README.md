# LLM Fine-tuning Project

æœ¬é¡¹ç›®åŸºäº **Qwen2.5-0.5B** æ¨¡å‹ï¼Œä½¿ç”¨ **Alpaca æŒ‡ä»¤æ•°æ®é›†** è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶æ‰‹åŠ¨å®ç°äº†ä¸€ä¸ªæ”¯æŒ **è´ªå©ªè§£ç  (Greedy Decoding)**ã€**é‡‡æ ·è§£ç  (Sampling)** ä»¥åŠ **æŸæœç´¢ (Beam Search)** çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°ã€‚

## ğŸ“‚ æ–‡ä»¶ç»“æ„ (Project Structure)

```text
.
â”œâ”€â”€ Qwen2.5-0.5B/             # é¢„è®­ç»ƒæ¨¡å‹æƒé‡ç›®å½•
â”œâ”€â”€ output/                   # è®­ç»ƒè¾“å‡ºç›®å½• (Checkpoint)
â”‚   â””â”€â”€ checkpoint-epoch-3/   # æœ€ç»ˆå¾®è°ƒåçš„æ¨¡å‹æƒé‡
â”œâ”€â”€ train.csv                 # Alpaca è®­ç»ƒæ•°æ®é›†
â”œâ”€â”€ train_inference.ipynb     # åŸå§‹ Jupyter Notebook ä»£ç 
â”œâ”€â”€ train_inference.py             # å¯¼å‡ºçš„ Python è®­ç»ƒä¸æ¨ç†è„šæœ¬
â”œâ”€â”€ train.log                 # è®­ç»ƒä¸æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ—¥å¿—
â””â”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸ› ï¸ æ ¸å¿ƒå®ç°é€»è¾‘ (Implementation Details)

### 1. æ•°æ®é¢„å¤„ç† (Data Preprocessing)
- **Tokenization**: 
  - å®ç°äº† `tokenize_function`ï¼Œåˆ©ç”¨ Qwen çš„ `chat_template` æ„å»ºå¯¹è¯æ ¼å¼ã€‚
  - **Loss Masking**: å·§å¦™è®¾ç½® `labels`ï¼Œå°† `input` (User Prompt) éƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º `-100`ï¼Œç¡®ä¿æ¨¡å‹åªè®¡ç®— `output` (Assistant Response) éƒ¨åˆ†çš„æŸå¤±ã€‚
  - **Left Padding**: ä¸ºæ¨ç†é˜¶æ®µçš„ç”Ÿæˆä»»åŠ¡é¢„å…ˆé…ç½®äº†å·¦ä¾§å¡«å……ï¼ˆLeft Paddingï¼‰ã€‚

- **Dynamic Padding (é™„åŠ é¢˜ 1)**:
  - å®ç°äº† `collate_fn`ï¼Œä½¿ç”¨ `torch.nn.utils.rnn.pad_sequence` å¯¹æ¯ä¸ª Batch çš„ `input_ids` å’Œ `labels` è¿›è¡ŒåŠ¨æ€å¡«å……ï¼Œå®ç°äº† `batch_size > 1` çš„é«˜æ•ˆè®­ç»ƒã€‚

### 2. æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ (Manual Training Loop)
- æ”¾å¼ƒä½¿ç”¨ `Trainer` APIï¼Œæ‰‹åŠ¨ç¼–å†™äº† PyTorch è®­ç»ƒå¾ªç¯ã€‚
- **Shift Logits**: åœ¨è®¡ç®— CrossEntropyLoss æ—¶ï¼Œæ‰‹åŠ¨å¯¹ `logits` è¿›è¡Œå·¦ç§»ï¼ˆå»æ‰æœ€åä¸€ä½ï¼‰ï¼Œå¯¹ `labels` è¿›è¡Œå·¦ç§»ï¼ˆå»æ‰ç¬¬ä¸€ä½ï¼‰ï¼Œç¬¦åˆ Causal LM çš„è‡ªå›å½’é¢„æµ‹ç‰¹æ€§ã€‚

### 3. è‡ªå®šä¹‰æ–‡æœ¬ç”Ÿæˆ (Custom Generation)
æ‰‹åŠ¨å®ç°äº† `generate` å‡½æ•°ï¼Œæ›¿ä»£ `model.generate`ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **Batch Generation (é™„åŠ é¢˜ 2)**:
  - æ”¯æŒ `List[str]` è¾“å…¥ã€‚
  - å¼ºåˆ¶ä½¿ç”¨ **Left Padding**ï¼Œé˜²æ­¢ Padding Token å¹²æ‰°ç”Ÿæˆç»“æœã€‚
  - å®ç°äº† **åå¤„ç†æˆªæ–­ (Post-processing)**ï¼Œé€šè¿‡æ£€æµ‹ `<|im_end|>` å’Œ `\nUser:` ç­‰å…³é”®è¯ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡å‹â€œè‡ªé—®è‡ªç­”â€çš„é—®é¢˜ã€‚

- **Sampling Strategy**:
  - å®ç°äº† **Top-k** å’Œ **Top-p (Nucleus)** é‡‡æ ·é€»è¾‘ã€‚
  - æ”¯æŒ **Temperature** è°ƒèŠ‚ç”Ÿæˆçš„éšæœºæ€§ã€‚
  - å®ç°äº† **Repetition Penalty** (é‡å¤æƒ©ç½š)ï¼Œæœ‰æ•ˆç¼“è§£äº†å°æ¨¡å‹ç”Ÿæˆä¹±ç æˆ–å¤è¯»æœºçš„é—®é¢˜ã€‚

- **Beam Search (é™„åŠ é¢˜ 3)**:
  - å®ç°äº†å®Œæ•´çš„æŸæœç´¢ç®—æ³•ã€‚
  - å¹¶è¡Œå¤„ç† `num_beams` ä¸ªå€™é€‰è·¯å¾„ã€‚
  - ç»´æŠ¤ `beam_scores`ï¼ˆå¯¹æ•°æ¦‚ç‡ç´¯åŠ ï¼‰ï¼Œå¹¶æ”¯æŒ `length_penalty`ï¼ˆé•¿åº¦æƒ©ç½šï¼‰ï¼Œæ˜¾è‘—æå‡äº†é•¿æ–‡æœ¬ç”Ÿæˆçš„é€»è¾‘æ€§å’Œå®Œæ•´æ€§ã€‚

## ğŸ“Š ç»“æœä¸åˆ†æ (Results & Analysis)

### 1. è´ªå©ªè§£ç  (Greedy Decoding)
> **æµ‹è¯•è¾“å…¥**: "Give me a brief introduction to Shanghai Jiao Tong University."

*   **è¡¨ç°**: æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…³äºä¸Šæµ·äº¤é€šå¤§å­¦çš„ä»‹ç»ï¼Œæ ¼å¼å·¥æ•´ã€‚
*   **å±€é™æ€§**: ç”±äºæ¨¡å‹å‚æ•°é‡è¾ƒå° (0.5B) ä¸”å¾®è°ƒæ•°æ®ä¸ºé€šç”¨é¢†åŸŸï¼Œæ¨¡å‹å­˜åœ¨ä¸€å®šçš„â€œå¹»è§‰â€ (Hallucination)ï¼Œä¾‹å¦‚å°†å»ºæ ¡å¹´ä»½è¯¯è®°ä¸º 1905 å¹´ï¼ˆå®é™…ä¸º 1896 å¹´ï¼‰ã€‚è¿™åæ˜ äº†å°æ¨¡å‹åœ¨çŸ¥è¯†å‡†ç¡®æ€§ä¸Šçš„å¤©ç„¶åŠ£åŠ¿ã€‚

### 2. é‡‡æ ·è§£ç  (Sampling Decoding)
> **æµ‹è¯•è¾“å…¥**: "Tell me a joke about computers." (é‡å¤ 3 æ¬¡)

*   **è¡¨ç°**: åœ¨å¼€å¯ `do_sample=True, temperature=0.7` åï¼Œæ¨¡å‹å±•ç°äº†å¤šæ ·æ€§ã€‚
*   **åˆ†æ**: 
    - å³ä½¿è¾“å…¥ç›¸åŒï¼Œæ¨¡å‹æ¯æ¬¡ç”Ÿæˆçš„ç¬‘è¯å†…å®¹æˆ–æªè¾å‡ä¸ç›¸åŒã€‚
    - æ¨¡å‹å¶å°”ä¼šæ¨¡ä»¿è®­ç»ƒæ•°æ®ä¸­çš„æ’ç‰ˆï¼ˆå¦‚åœ¨é—®ç­”é—´æ’å…¥ç©ºè¡Œï¼‰ï¼Œè¿™è¯æ˜é‡‡æ ·ç­–ç•¥æˆåŠŸå¼•å…¥äº†åˆ†å¸ƒçš„éšæœºæ€§ï¼Œå¢åŠ äº†ç”Ÿæˆçš„è¶£å‘³æ€§ã€‚

### 3. æŸæœç´¢ (Beam Search)
> **æµ‹è¯•è¾“å…¥**: "What is the sum of the first 100 natural numbers? Please think step by step."

*   **è¡¨ç°**: æ•ˆæœæœ€ä½³ã€‚
*   **åˆ†æ**: 
    - æ¨¡å‹å‡†ç¡®å¼•ç”¨äº†æ±‚å’Œå…¬å¼ `n(n+1)/2` å¹¶è®¡ç®—å‡ºäº†æ­£ç¡®ç»“æœ `5050`ã€‚
    - ç›¸æ¯”è´ªå©ªæœç´¢ï¼ŒBeam Search åœ¨åœæ­¢ç”Ÿæˆçš„æ—¶æœºä¸ŠæŠŠæ¡å¾—æ›´ç²¾å‡†ï¼Œæ²¡æœ‰å‡ºç°å¤šä½™çš„åºŸè¯ã€‚è¿™è¯æ˜äº†é€šè¿‡ä¿ç•™å¤šæ¡å€™é€‰è·¯å¾„ï¼Œæ¨¡å‹èƒ½å¤Ÿè§„åˆ’å‡ºå…¨å±€æ¦‚ç‡æ›´é«˜çš„æœ€ä¼˜è§£ã€‚

## ğŸš€ å¦‚ä½•è¿è¡Œ (How to Run)

### ç¯å¢ƒä¾èµ–
ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š
```bash
pip install torch transformers datasets accelerate
```

### è¿è¡Œè®­ç»ƒä¸æ¨ç†
å»ºè®®åœ¨æœåŠ¡å™¨åå°è¿è¡Œï¼Œå¹¶å°†è¾“å‡ºé‡å®šå‘è‡³æ—¥å¿—æ–‡ä»¶ï¼š
```bash
python -u train_inference.py > train.log 2>&1
```

### æŸ¥çœ‹å®æ—¶æ—¥å¿—
```bash
tail -f train.log
```
