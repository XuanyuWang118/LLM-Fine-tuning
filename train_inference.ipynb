{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 第一部分：使用指令数据对基底模型进行有监督微调\n","在本作业的第一部分，我们将使用Qwen2.5-0.5B基底模型以及alpaca指令数据集，体验如何对LLM做指令微调的训练。\n","\n","> 关于Transformer的基本使用教程，可以参考官方推出的[LLM Course](https://huggingface.co/learn/llm-course/chapter2/3)。本次作业要求同学们手写训练代码，不能使用里面提供的Trainer API，关于如何使用PyTorch训练模型，可以参照[这个教程](https://huggingface.co/docs/transformers/v4.49.0/en/training#train-in-native-pytorch)。\n","\n","> 对于使用Kaggle进行作业的同学，这里有一份[Kaggle基础使用](https://www.kaggle.com/code/cnlnpjhsy/kaggle-transformers)的简单教学供参考。"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# 如果缺失必要的库，可以使用下面的命令安装\n","# !pip install torch transformers datasets accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/exp/xuanyu.wang/espnet_20250624/tools/miniconda/envs/lecture/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import datasets"]},{"cell_type":"markdown","metadata":{},"source":["## 加载模型、tokenizer与数据集\n","本次作业，我们使用通义千问的Qwen2.5-0.5B预训练模型进行微调。对于在本地部署的同学，请事先将模型文件下载到本地；对于在kaggle上进行作业的同学，可以依照kaggle上的教程，将`MODEL_PATH`与`DATASET_PATH`修改为Input中的路径。"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(151936, 896)\n","    (layers): ModuleList(\n","      (0-23): 24 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n","          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n","          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n","          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n","          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n","          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n","        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n","    (rotary_emb): Qwen2RotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",")\n","{'instruction': 'Give three tips for staying healthy.', 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': None}\n","{'instruction': 'What are the three primary colors?', 'output': 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).', 'input': None}\n","{'instruction': 'Describe the structure of an atom.', 'output': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\", 'input': None}\n","{'instruction': 'How can we reduce air pollution?', 'output': 'There are several ways to reduce air pollution, including:\\n\\n1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\\n\\n2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\\n\\n3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\\n\\n4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\\n\\n5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\\n\\n6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air.', 'input': None}\n","{'instruction': 'Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.', 'input': None}\n","{'instruction': 'Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"', 'output': 'The Commodore 64 was a highly successful 8-bit home computer manufactured by Commodore Business Machine (CBM) in 1982, with sales amounting to approximately 17 million units sold between 1983-1986. It dominated the market with between 30% and 40% share and outsold its competitors, including IBM PC clones, Apple Computers, and Atari computers. At its peak, CBM was building 400,000 C64s a month for a couple of years.', 'input': None}\n","{'instruction': 'Explain why the following fraction is equivalent to 1/4', 'output': 'The fraction 4/16 is equivalent to 1/4 because both fractions represent the same value. A fraction can be simplified by dividing both the numerator and the denominator by a common factor. In this case, 4 is a common factor of both the numerator and the denominator of 4/16. When we divide both by 4, we get 4/4 = 1 and 16/4 = 4, so the simplified fraction is 1/4. Alternatively, we can think of this in terms of multiplication. For example, if we multiply the numerator and denominator of the fraction 1/4 by 4, we get (1x4)/(4x4), or 4/16. Since both fractions can be derived from the other through multiplication or division by the same number, they represent the same value and are equivalent.', 'input': '4/16'}\n","{'instruction': 'Write a short story in third person narration about a protagonist who has to make an important career decision.', 'output': \"Sophie sat at her desk, staring blankly at the computer screen. Her mind was racing as she weighed the options in front of her. She had been offered a promotion to a senior management position within her company, but it meant moving across the country and leaving her friends and family behind. At the same time, her dream job at a rival company had finally opened up, and she had been invited for an interview. \\n\\nThe thought of leaving her comfortable life behind, to take a chance on something new and uncertain, made Sophie feel anxious, but also excited. She had always been one to take risks, and it had paid off in the past. But now, with such a big decision to make, Sophie couldn't help but feel overwhelmed.\\n\\nAfter much deliberation, Sophie made her choice. She packed her bags, said goodbye to her loved ones, and moved across the country for the new job. It was a difficult first few months, adjusting to a new city and a new company culture. But as time went by, Sophie realized that she had made the best decision for her career. The job was challenging, but also rewarding, and she was surrounded by passionate, driven colleagues who inspired her to grow and develop.\\n\\nIn the end, Sophie's bold move paid off. She was thriving in her new role, and had made new friends and connections in her new city. Looking back, Sophie was grateful for the opportunity to take a risk and make a change, and was confident that she had made the right decision for her career.\", 'input': None}\n","{'instruction': 'Evaluate this sentence for spelling and grammar mistakes', 'output': 'There are two spelling errors in the sentence. The corrected sentence should be: \"He finished his meal and left the restaurant.\"', 'input': 'He finnished his meal and left the resturant'}\n","{'instruction': 'How did Julius Caesar die?', 'output': \"Julius Caesar, the Roman Military general, and statesman was assassinated on March 15, 44 BC. He was stabbed to death at a meeting of the Senate in the Theatre of Pompey by a group of senators who called themselves the Liberatores. Led by Gaius Cassius Longinus, and Marcus Junius Brutus, they believed that Caesar's dictatorship was subverting the Roman Republic and sought to restore the traditional republican government. The exact number of assailants is not known, but it is believed that a group of as many as 60 conspirators participated in the assassination, and each of them took turns in stabbing Caesar. The day of his death is still known as the Ides of March.\", 'input': None}\n"]}],"source":["MODEL_PATH = \"Qwen2.5-0.5B\"\n","DATASET_PATH = \"train.csv\"\n","\n","model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", dtype=\"auto\")\n","print(model)\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","dataset = datasets.Dataset.from_csv(DATASET_PATH)\n","for sample in dataset.select(range(10)):    # 查看前10个样本。思考应该怎么将样本组织成单条完整文本？\n","    print(sample)"]},{"cell_type":"markdown","metadata":{},"source":["Qwen为基底模型也提供了对话模板（chat template），对话模板中含有一些特殊的token，可以帮助我们区分说话人的轮次（思考一下为什么要区分？）。我们可以直接以下述“轮次对话”的方式，构造一个样例文本。"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nThis is a question.<|im_end|>\\n<|im_start|>assistant\\nI'm the answer!<|im_end|>\\n\""]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.apply_chat_template([\n","    {\"role\": \"user\", \"content\": \"This is a question.\"},\n","    {\"role\": \"assistant\", \"content\": \"I'm the answer!\"}\n","], tokenize=False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["可以看到每一轮次的对话都以`<|im_end|>`这个token结束。但是基底模型是没有在对话上经过优化的，它并不认得这个终止符。因此我们需要修改tokenizer的终止符，使其知道什么token代表一个对话轮次的结束。"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<|endoftext|>\n"]}],"source":["print(tokenizer.eos_token)  # 原来的终止符\n","tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","model.generation_config.eos_token_id = tokenizer.eos_token_id  # 也要修改模型的终止符"]},{"cell_type":"markdown","metadata":{},"source":["为了与训练后的模型做对比，我们先使用模型自带的generate方法测试一下这个基底模型会生成什么样的文本："]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Shanghai Jiao Tong University (JTU) is a public university in Shanghai, China. It was founded in 1905 and is one of the oldest universities in China. The university has a rich history and is known for its academic excellence and research in various fields. It has a strong focus on interdisciplinary studies and offers a wide range of undergraduate and graduate programs. The university is also known for its commitment to social responsibility and has a strong alumni network. Overall, Shanghai Jiao Tong University is a highly regarded institution that has made significant contributions to the development of China's education system.\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","-transitional\n","\n"]}],"source":["messages = [\n","    {\"role\": \"user\", \"content\": \"Give me a brief introduction to Shanghai Jiao Tong University.\"},\n","]\n","text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","with torch.no_grad():\n","    lm_inputs_src = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n","    generate_ids = model.generate(**lm_inputs_src, max_new_tokens=150, do_sample=False)\n","pred_str = tokenizer.decode(generate_ids[0][lm_inputs_src.input_ids.size(1):], skip_special_tokens=True)\n","print(pred_str)"]},{"cell_type":"markdown","metadata":{},"source":["## 处理数据集\n","原始的alpaca数据集是纯文本形式，而非模型能够接受的token。我们需要先将这些文本tokenize，再传给模型。\n","\n","在指令微调阶段，我们常常希望模型只在模型要生成回答的部分上做优化，而不在问题文本上做训练，这需要我们特别设计传入的标签。请完成下述的`tokenize_function`函数，将数据集的指令样本tokenize，并传回输入模型的`input_ids`以及用于<b>仅在output部分计算损失</b>的标签`labels`。"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import copy\n","def tokenize_function(sample):\n","    # 构建对话消息列表\n","    # 注意：Alpaca数据集通常包含 instruction, input (可选), output\n","    prompt_content = sample[\"instruction\"]\n","    if sample.get(\"input\", \"\"): # 如果有input字段且不为空，拼接到instruction后面\n","        prompt_content += \"\\n\" + sample[\"input\"]\n","    \n","    # 1. 构建 Prompt 部分（User）\n","    messages_prompt = [\n","        {\"role\": \"user\", \"content\": prompt_content},\n","    ]\n","    # 生成 Prompt 的文本（添加 generation prompt 标记，如 <|im_start|>assistant\\n）\n","    prompt_text = tokenizer.apply_chat_template(messages_prompt, tokenize=False, add_generation_prompt=True)\n","    \n","    # 2. 构建 完整对话 部分（User + Assistant）\n","    messages_full = [\n","        {\"role\": \"user\", \"content\": prompt_content},\n","        {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n","    ]\n","    # 生成完整对话文本\n","    full_text = tokenizer.apply_chat_template(messages_full, tokenize=False)\n","    \n","    # 3. 将文本转换为 token ids\n","    # 注意：这里我们分别对 prompt 和 full_text 进行 tokenize，是为了计算 prompt 的长度\n","    prompt_ids = tokenizer(prompt_text, add_special_tokens=False).input_ids\n","    full_ids = tokenizer(full_text, add_special_tokens=False).input_ids\n","    \n","    # Qwen2.5 的 tokenizer 可能会在开头自动添加 text_ids，为保险起见，建议加上 add_special_tokens=False\n","    # 并在之前手动加上 tokenizer.bos_token_id (如果有的话)，但 apply_chat_template 通常处理好了\n","    \n","    # 4. 构建 Labels\n","    # 初始化 labels 为 full_ids 的副本\n","    labels = copy.deepcopy(full_ids)\n","    \n","    # 将 prompt 部分的 labels 设置为 -100，这样计算 loss 时会被忽略\n","    # 注意：我们要忽略的是 prompt_ids 长度的部分\n","    prompt_len = len(prompt_ids)\n","    \n","    # 这里的切片处理要小心，确保长度一致。\n","    # 只要 prompt_text 是 full_text 的前缀，这种长度截断就是安全的。\n","    if len(full_ids) > prompt_len:\n","        labels[:prompt_len] = [-100] * prompt_len\n","    else:\n","        # 异常保护：如果full比prompt还短（极少见），全部忽略\n","        labels = [-100] * len(labels)\n","\n","    # 显式添加 EOS token (如果 apply_chat_template 没有加，通常它会加，但 Qwen 需要确认)\n","    # 之前的代码已经设置了 tokenizer.eos_token_id，这里不做额外操作，依赖 template 结果。\n","    \n","    input_ids = full_ids\n","    \n","    return {\"input_ids\": input_ids, \"labels\": labels}\n","\n","tokenized_dataset = dataset.map(\n","    tokenize_function, remove_columns=dataset.column_names\n",").filter(\n","    lambda x: len(x[\"input_ids\"]) <= 512\n",")"]},{"cell_type":"markdown","metadata":{},"source":["定义一个DataLoader，用于从中获取模型能够处理的tokenized输入。  \n","> <b>【附加1】（3分）</b>通过从dataloader中成批取出数据，可以提升计算效率。你能够设计`collate_fn`，使之能以`batch_size > 1`的方式获取数据吗？"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_fn(batch):\n","    # 提取 batch 中的 input_ids 和 labels\n","    # batch 是一个 list，里面每个元素是 tokenize_function 返回的 dict\n","    input_ids_list = [torch.tensor(item[\"input_ids\"]) for item in batch]\n","    labels_list = [torch.tensor(item[\"labels\"]) for item in batch]\n","    \n","    # 1. 对 input_ids 进行 padding\n","    # batch_first=True 表示返回 (batch_size, seq_len)\n","    # padding_value 使用 tokenizer.pad_token_id\n","    input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n","    \n","    # 2. 对 labels 进行 padding\n","    # padding_value 使用 -100 (计算 Loss 时忽略)\n","    labels = pad_sequence(labels_list, batch_first=True, padding_value=-100)\n","    \n","    # 3. 生成 attention_mask\n","    # input_ids 不等于 pad_token_id 的地方为 1，否则为 0\n","    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n","    \n","    return {\n","        \"input_ids\": input_ids, \n","        \"attention_mask\": attention_mask, \n","        \"labels\": labels\n","    }\n","\n","# 根据显存占用情况，可以适当调整batch_size\n","train_dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"]},{"cell_type":"markdown","metadata":{},"source":["## 训练模型\n","准备好tokenized后的数据后，就可以对模型进行训练了。请手动编写用于训练的循环，计算损失并反传。\n","\n","在向model传入labels时，Transformer模型内部会自动计算损失；但为了让同学们理解损失的内部计算机制，我们要求**不向模型forward中传入labels，而是手动将模型的最终输出logits与labels相比对，并计算损失。**  \n","> <b>【附加1】</b>从dataloader中成批获取数据后，要将整个batch一次性输入到模型中（并非是使用循环逐个处理批次输入），获取所有样例的loss，并正确计算损失。"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# # from tqdm.notebook import tqdm\n","# from tqdm import tqdm\n","# from torch.optim import AdamW\n","# import torch.nn as nn\n","\n","# step = 0\n","# # TODO: 定义你的优化器与损失函数\n","# # 1. 定义优化器与损失函数\n","# # 学习率通常设置较小，如 1e-5 或 5e-5\n","# optimizer = AdamW(model.parameters(), lr=1e-5) \n","\n","# # CrossEntropyLoss，设置 ignore_index=-100 以忽略 padding 和 prompt 部分\n","# loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n","\n","# model.train()\n","# # 将模型移动到 GPU (如果之前定义时 device_map=\"auto\" 已经移过去了，这里确保一下)\n","# device = model.device \n","\n","# for epoch in range(3):\n","#     for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n","#         # 将数据移动到设备上\n","#         input_ids = batch[\"input_ids\"].to(device)\n","#         attention_mask = batch[\"attention_mask\"].to(device)\n","#         labels = batch[\"labels\"].to(device)\n","        \n","#         # 清空梯度\n","#         optimizer.zero_grad()\n","        \n","#         # 2. 前向传播 (不传入 labels)\n","#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","#         logits = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n","        \n","#         # 3. 手动计算 Loss (Shift Logits)\n","#         # 语言模型的特性：第 t 个 token 的 logits 用于预测第 t+1 个 token\n","#         # 因此，我们需要将 logits 向左平移一位（去掉最后一个），将 labels 向左平移一位（去掉第一个）\n","        \n","#         # shift_logits: [batch_size, seq_len-1, vocab_size]\n","#         shift_logits = logits[..., :-1, :].contiguous()\n","#         # shift_labels: [batch_size, seq_len-1]\n","#         shift_labels = labels[..., 1:].contiguous()\n","        \n","#         # 将 tensor 展平以适配 CrossEntropyLoss\n","#         # view(-1, ...) 相当于 flatten\n","#         loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","        \n","#         # 4. 反向传播与优化\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         step += 1\n","#         if step % 100 == 0:\n","#             print(f\"Step {step}\\t| Loss: {loss.item()}\")\n","            \n","#     # 保存逻辑保持不变\n","#     model.save_pretrained(f\"output/checkpoint-epoch-{epoch + 1}\")\n","#     tokenizer.save_pretrained(f\"output/checkpoint-epoch-{epoch + 1}\")"]},{"cell_type":"markdown","metadata":{},"source":["测试训练后的模型效果。如果训练正常，模型应当能回答出通顺的语句，并在回答结束后自然地停止生成。"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Shanghai Jiao Tong University (JTI) is a public research university located in Shanghai, China. It was founded in 1905 and is one of the oldest universities in China. JTI is known for its strong emphasis on research and innovation, and its faculty and students are highly regarded in the field of science and engineering. The university has a diverse range of programs and offers a wide range of courses in fields such as engineering, business, and social sciences. It is also home to several prestigious research centers and institutes, including the Shanghai Institute of Microbiology and Immunology, the Shanghai Institute of Oceanology, and the Shanghai Institute of Oceanology and Ocean Engineering. In addition to its academic programs, JTI also offers a range\n"]}],"source":["sft_model = AutoModelForCausalLM.from_pretrained(\"output/checkpoint-epoch-3\", device_map=\"auto\", dtype=\"auto\")\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Give me a brief introduction to Shanghai Jiao Tong University.\"},\n","]\n","text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","with torch.no_grad():\n","    lm_inputs_src = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(sft_model.device)\n","    generate_ids = sft_model.generate(**lm_inputs_src, max_new_tokens=150, do_sample=False)\n","pred_str = tokenizer.decode(generate_ids[0][lm_inputs_src.input_ids.size(1):], skip_special_tokens=True)\n","print(pred_str)"]},{"cell_type":"markdown","metadata":{},"source":["如果模型行为正常，就可以继续前往大作业的第二部分了！"]},{"cell_type":"markdown","metadata":{},"source":["# 第二部分：使用LLM做推理生成，并解码为自然文本\n","在这一部分，我们将体验LLM是如何逐token进行生成、并解码出自然文本的。我们需要手动实现一个`generate`函数，它能够直接接受用户的自然文本作为输入，并同样以自然文本回复。"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The tokenizer you are loading from 'output/checkpoint-epoch-3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"]}],"source":["MODEL_PATH = \"output/checkpoint-epoch-3\"    # 你训练好的模型路径\n","\n","model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", dtype=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","model.generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{},"source":["## 实现generate\n","请实现下述的generate函数，手动进行模型推理、生成与解码。\n","\n","这个generate函数至少能够接受一个字符串`query`作为输入，限制最大生成token数`max_new_tokens`，并用`do_sample`选择是采用采样还是贪婪搜索进行生成。在使用采样策略生成时，允许设置基础的采样生成参数`temperature`、`top_p`和`top_k`。关于不同的生成策略是如何工作的，可以学习这篇[博客](https://huggingface.co/blog/how-to-generate)。  \n","**禁止使用模型自带的`model.generate`方法！**\n","\n","> <b>附加2（3分）</b>你能够利用模型的批次输入特性（并非是使用循环逐个处理批次输入），成批次地输入文本、并同时生成新token吗？此时`query`应该可以接受一个字符串列表作为输入。\n","\n","> <b>附加3（3分）</b>束搜索（Beam search）允许在解码过程中保留数个次优序列，通过生成过程中维护这些序列，模型能够生成整体更为合理的句子，改善了贪婪搜索中可能会陷入局部最优的问题。你可以在已有的贪婪搜索与采样两种生成策略的基础上实现束搜索吗？此时`num_beams`应允许大于1的值。  \n","关于束搜索，这里有一个[可视化Demo](https://huggingface.co/spaces/m-ric/beam_search_visualizer)演示其运作机理。"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from typing import Union, List\n","\n","def post_process_response(text):\n","    # 1. 优先匹配明确的特殊 Token (Qwen 的标准结束符)\n","    # 注意：decode后，<|im_end|> 可能会变成字符串形式\n","    special_stop_patterns = [\"<|im_end|>\", \"<|im_start|>\"]\n","    for pattern in special_stop_patterns:\n","        if pattern in text:\n","            text = text.split(pattern)[0]\n","\n","    # 2. 匹配文本模式的自问自答 (这是你遇到的主要问题)\n","    # 比如模型自己生成了 \"\\nUser:\" 或者 \"\\nInput:\"\n","    text_stop_patterns = [\n","        \"\\nUser:\", \"\\nuser:\", \n","        \"\\nInput:\", \"\\ninput:\",\n","        \"\\nQ:\", \"\\nQuestion:\",\n","        \"\\n问：\", \"\\n问题：\"\n","    ]\n","    \n","    for pattern in text_stop_patterns:\n","        # 使用 rsplit 还是 split? 通常我们只关心第一次出现\n","        if pattern in text:\n","            # 找到模式出现的位置\n","            idx = text.find(pattern)\n","            # 只要这个模式出现了，且不是在开头（避免把刚生成的答案全切了），就截断\n","            if idx > 0: \n","                text = text[:idx]\n","    \n","    # 3. 处理可能存在的乱码结尾（如 riott 这种孤立词）\n","    # 这一步比较激进，视情况使用。简单的方法是再做一次 strip\n","    return text.strip()\n","\n","def generate(\n","    model: AutoModelForCausalLM,\n","    query: Union[str, List[str]],\n","    max_new_tokens: int = 1024,\n","    do_sample: bool = False,\n","    temperature: float = 1.0,\n","    top_p: float = 0.9,\n","    top_k: int = 50,\n","    num_beams: int = 1,\n","    length_penalty: float = 1.0,\n",") -> Union[str, List[str]]:\n","    \n","    # --- 1. 数据预处理与Batch构造 ---\n","    # 统一将输入转为 List 处理\n","    is_single_input = isinstance(query, str)\n","    queries = [query] if is_single_input else query\n","    batch_size = len(queries)\n","    device = model.device\n","\n","    # 构造 Chat 模板输入\n","    formatted_queries = []\n","    for q in queries:\n","        messages = [{\"role\": \"user\", \"content\": q}]\n","        # add_generation_prompt=True 会添加 <|im_start|>assistant\\n\n","        formatted_queries.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n","\n","    # Tokenize\n","    # 关键点：生成任务必须使用 Left Padding，因为输出是在右侧生成的\n","    tokenizer.padding_side = \"left\" \n","    inputs = tokenizer(formatted_queries, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","    \n","    input_ids = inputs.input_ids\n","    attention_mask = inputs.attention_mask\n","    input_len = input_ids.shape[1]\n","\n","    # 设置结束符 ID\n","    eos_token_id = tokenizer.eos_token_id\n","    if isinstance(eos_token_id, list): eos_token_id = eos_token_id[0]\n","\n","    # --- 分支：Beam Search 还是 普通解码 ---\n","    if num_beams > 1:\n","        # --- 附加3：Beam Search 实现 ---\n","        # 1. Expand inputs: (batch, seq) -> (batch * beams, seq)\n","        # 这样我们可以并行处理所有的 beam\n","        input_ids = input_ids.repeat_interleave(num_beams, dim=0)\n","        attention_mask = attention_mask.repeat_interleave(num_beams, dim=0)\n","        \n","        # 初始化分数：每个样本的第一个 beam 分数为 0，其余为 -inf (保证第一次只从第一个beam扩展)\n","        beam_scores = torch.zeros((batch_size, num_beams), device=device)\n","        beam_scores[:, 1:] = -1e9\n","        beam_scores = beam_scores.view(-1)  # (batch * beams)\n","\n","        # 记录生成的序列\n","        generated_sequences = input_ids\n","        \n","        # 记录完成的序列 (batch_size, num_beams)\n","        # finished_sequences 存储 (score, sequence)\n","        finished_sequences = [[] for _ in range(batch_size)] \n","        \n","        cur_len = input_len\n","        \n","        for _ in range(max_new_tokens):\n","            with torch.no_grad():\n","                outputs = model(input_ids=generated_sequences, attention_mask=attention_mask)\n","                next_token_logits = outputs.logits[:, -1, :]  # (batch * beams, vocab)\n","\n","            # 计算 log_softmax\n","            next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch * beams, vocab)\n","            \n","            # 累加分数: previous_score + current_score\n","            # beam_scores: (batch * beams, 1)\n","            next_scores = beam_scores.unsqueeze(-1) + next_token_scores # (batch * beams, vocab)\n","            \n","            # Reshape 以便在每个 batch 内部进行 topk\n","            # (batch, beams * vocab)\n","            next_scores = next_scores.view(batch_size, -1)\n","            \n","            # 取出每个 batch 中分数最高的 2 * num_beams 个候选 (为了留余量给已完成的)\n","            topk_scores, topk_indices = torch.topk(next_scores, 2 * num_beams, dim=1)\n","            \n","            # 解析索引：beam_idx 和 token_idx\n","            # indices 范围是 [0, beams * vocab - 1]\n","            beam_indices = topk_indices // model.config.vocab_size\n","            token_indices = topk_indices % model.config.vocab_size\n","            \n","            # 构建下一轮的输入\n","            next_beam_scores = []\n","            next_generated_sequences = []\n","            next_attention_mask = []\n","\n","            for batch_idx in range(batch_size):\n","                if len(finished_sequences[batch_idx]) >= num_beams:\n","                    # 该样本已找齐，随便填点东西占位（最后会被忽略）\n","                    next_beam_scores.extend([-1e9] * num_beams)\n","                    next_generated_sequences.extend([generated_sequences[batch_idx * num_beams]] * num_beams)\n","                    next_attention_mask.extend([attention_mask[batch_idx * num_beams]] * num_beams)\n","                    continue\n","\n","                valid_beams_count = 0\n","                for i in range(2 * num_beams):\n","                    score = topk_scores[batch_idx, i].item()\n","                    token = token_indices[batch_idx, i].item()\n","                    beam_idx = beam_indices[batch_idx, i].item() # 0 ~ num_beams-1\n","                    \n","                    # 真正的全局 index\n","                    global_beam_idx = batch_idx * num_beams + beam_idx\n","                    \n","                    if token == eos_token_id:\n","                        # 句子结束，加入结果集\n","                        # 长度惩罚： score / (len ** penalty)\n","                        final_score = score / ((cur_len - input_len + 1) ** length_penalty)\n","                        finished_sequences[batch_idx].append((final_score, torch.cat([generated_sequences[global_beam_idx], torch.tensor([token], device=device)])))\n","                    else:\n","                        # 句子未结束，加入下一轮候选\n","                        if valid_beams_count < num_beams:\n","                            next_beam_scores.append(score)\n","                            new_seq = torch.cat([generated_sequences[global_beam_idx], torch.tensor([token], device=device)])\n","                            next_generated_sequences.append(new_seq)\n","                            new_mask = torch.cat([attention_mask[global_beam_idx], torch.tensor([1], device=device)])\n","                            next_attention_mask.append(new_mask)\n","                            valid_beams_count += 1\n","            \n","            # 更新状态\n","            beam_scores = torch.tensor(next_beam_scores, device=device)\n","            generated_sequences = torch.stack(next_generated_sequences)\n","            attention_mask = torch.stack(next_attention_mask)\n","            \n","            cur_len += 1\n","            \n","            # 检查是否所有 batch 都完成了\n","            if all([len(fs) >= num_beams for fs in finished_sequences]):\n","                break\n","        \n","        # 整理输出结果：取分数最高的那一条\n","        final_sequences = []\n","        for batch_idx in range(batch_size):\n","            # 如果没生成完（比如超长），就把当前还在跑的最高分拿出来\n","            if len(finished_sequences[batch_idx]) == 0:\n","                 final_sequences.append(generated_sequences[batch_idx * num_beams])\n","            else:\n","                # 按分数排序\n","                finished_sequences[batch_idx].sort(key=lambda x: x[0], reverse=True)\n","                final_sequences.append(finished_sequences[batch_idx][0][1])\n","        \n","        output_ids = torch.nn.utils.rnn.pad_sequence(final_sequences, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    else:\n","        # --- 默认：Greedy / Sample 解码 ---\n","        # 复制一份 input_ids 用于拼接生成结果\n","        generated_ids = input_ids.clone()\n","        \n","        # 用于记录每个样本是否已经生成结束\n","        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n","\n","        for _ in range(max_new_tokens):\n","            # 1. 前向传播\n","            # 这里为了简单，每次都传入完整的 sequence。\n","            # 实际上可以使用 past_key_values (KV Cache) 来加速，但代码会复杂很多。\n","            with torch.no_grad():\n","                outputs = model(input_ids=generated_ids, attention_mask=attention_mask)\n","            \n","            # 2. 获取最后一个 token 的 logits\n","            next_token_logits = outputs.logits[:, -1, :] # (batch_size, vocab_size)\n","\n","            # 3. 采样策略处理\n","            if do_sample:\n","                # Temperature\n","                if temperature != 1.0 and temperature > 0:\n","                    next_token_logits = next_token_logits / temperature\n","                \n","                # Softmax 转概率\n","                probs = F.softmax(next_token_logits, dim=-1)\n","                \n","                # Top-K 过滤\n","                if top_k > 0:\n","                    # 获取前k个值的阈值\n","                    top_k_values, _ = torch.topk(probs, top_k)\n","                    min_top_k = top_k_values[:, -1].unsqueeze(-1)\n","                    # 低于阈值的设为 0\n","                    probs[probs < min_top_k] = 0\n","                    probs = probs / probs.sum(dim=-1, keepdim=True) # 归一化\n","\n","                # Top-P (Nucleus) 过滤\n","                if top_p < 1.0:\n","                    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n","                    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n","\n","                    # 找出累积概率超过 top_p 的位置\n","                    sorted_indices_to_remove = cumulative_probs > top_p\n","                    # 需要保留第一个超过 top_p 的 token，所以要把 mask 向右移一位\n","                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","                    sorted_indices_to_remove[..., 0] = 0\n","\n","                    # 恢复原序列顺序的 mask\n","                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n","                    probs[indices_to_remove] = 0\n","                    probs = probs / probs.sum(dim=-1, keepdim=True) # 归一化\n","                \n","                # 随机采样\n","                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n","            else:\n","                # 贪婪搜索：直接取概率最大的\n","                next_tokens = torch.argmax(next_token_logits, dim=-1)\n","\n","            # 4. 更新生成的序列\n","            # 如果该样本已经结束（EOS），则后续生成的 token 保持为 EOS 或 pad，不影响逻辑\n","            # 但为了保持长度一致，我们通常还是继续 append，只是最后 decode 时截断\n","            \n","            # 处理已经结束的句子，保持为 pad_token\n","            # next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n","            \n","            # 拼接\n","            next_tokens = next_tokens.unsqueeze(-1)\n","            generated_ids = torch.cat([generated_ids, next_tokens], dim=-1)\n","            \n","            # 更新 mask (新生成的token也需要被关注)\n","            attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1), device=device, dtype=attention_mask.dtype)], dim=-1)\n","\n","            # 5. 检查是否结束\n","            # 如果生成的 token 是 eos_token_id，标记为结束\n","            # Qwen 的 eos 可能是 list，这里假设是 int\n","            unfinished_sequences = unfinished_sequences.mul((next_tokens.squeeze() != eos_token_id).long())\n","            \n","            if unfinished_sequences.max() == 0:\n","                break\n","        \n","        output_ids = generated_ids\n","\n","    # --- 3. 解码与后处理 ---\n","    # 截取生成的 output 部分（去掉输入的 prompt 部分）\n","    # 注意：因为 input_ids 做过 padding，不同样本的 prompt 长度可能在 tensor 里是不对齐的（虽然左padding对齐了末尾）\n","    # 但最简单的做法是 decode 整个序列，然后按 string 匹配去掉 prompt，或者利用 input_len 统一截断\n","    \n","    # 这里我们只返回生成的“新”token对应的文本\n","    # 由于是 Left Padding，input 的有效长度是 input_len\n","    generated_only = output_ids[:, input_len:]\n","    \n","    # decoded_outputs = tokenizer.batch_decode(generated_only, skip_special_tokens=True)\n","    # # 返回结果\n","    # if is_single_input:\n","    #     return decoded_outputs[0]\n","    # else:\n","    #     return decoded_outputs\n","\n","    # 【关键修改】：这里改为 False，保留特殊字符以便后处理函数能识别 <|im_end|>\n","    decoded_outputs = tokenizer.batch_decode(generated_only, skip_special_tokens=False)\n","    \n","    clean_results = []\n","    for text in decoded_outputs:\n","        # 1. 先进行截断处理\n","        processed_text = post_process_response(text)\n","        \n","        # 2. 如果截断后还残留其他特殊 token (比如 <|endoftext|> 等)，再清洗一次\n","        # 这里可以使用 replace 把残留的特殊 token 删掉，或者重新 encode 再 decode(skip=True)\n","        # 简单做法是手动 replace 常见的\n","        for special in [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"]:\n","            processed_text = processed_text.replace(special, \"\")\n","            \n","        clean_results.append(processed_text.strip())\n","    \n","    # 返回结果\n","    if is_single_input:\n","        return clean_results[0]\n","    else:\n","        return clean_results"]},{"cell_type":"markdown","metadata":{},"source":["## 测试generate的效果\n","请同学们运行下述单元格，测试你的实现。除了下面提到的句子，同学们也可以自定义更多情况下的输入文本，探究模型在面对不同输入时采用不同解码策略的表现。"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["==================== #1 贪心解码 (Batch Generation) ====================\n","[0] 问：Give me a brief introduction to Shanghai Jiao Tong University.\n","    答：Shanghai Jiao Tong University (JTI) is a public research university located in Shanghai, China. It was established in 1905 and is one of the oldest universities in China. JTI is known for its strong emphasis on research and innovation, and its faculty and students are highly regarded in the field of science and engineering. The university has a diverse range of programs and offers a wide range of courses in fields such as engineering, business, and social sciences. It is also home to several prestigious research centers and institutes, including the Shanghai Institute of Microbiology and Immunology, the Shanghai Institute of Oceanology, and the Shanghai Institute of Oceanology and Ocean Engineering. In addition to its academic programs, JTI also offers a range of extracurricular activities and programs, including the Shanghai International Science and Technology Festival, the Shanghai International Science and Technology Exhibition, and the Shanghai International Science and Technology Conference. Overall, Shanghai Jiao Tong University is a highly respected institution that is committed to excellence in research and innovation.\n","--------------------------------------------------\n","[1] 问：介绍一下上海交通大学。\n","    答：上海交通大学（Shanghai Jiao Tong University, SJTU）是中华人民共和国教育部直属的全国重点综合性大学，位于中国上海市。学校创建于1911年，是中国最早建立的高等学府之一，也是中国历史上最早设立的工科大学之一。上海交通大学拥有丰富的学术资源和高水平的师资力量，其学科涵盖多个领域，包括工学、理学、医学、文学、法学、教育学、艺术学等。学校在国内外享有很高的声誉，是全球著名的综合性研究型大学之一。\n","--------------------------------------------------\n","[2] 问：What is the capital of China?\n","    答：The capital of China is Beijing.\n","--------------------------------------------------\n","\n","==================== #2 采样解码 (Sampling) ====================\n","[1] 问：Tell me a joke about computers.\n","    答：Why don't computers like cats? They're too small, they can't jump.\n","--------------------------------------------------\n","[2] 问：Tell me a joke about computers.\n","    答：Why don't scientists trust computers? They say, \"Well, they could be a little bit smart, but they're still just a bunch of transistors.\"\n","--------------------------------------------------\n","[3] 问：Tell me a joke about computers.\n","    答：Why did the computer go to therapy? It was feeling isolated.\n","--------------------------------------------------\n","\n","==================== #3 【附加3】束搜索解码 (Beam Search) ====================\n","问：What is the sum of the first 100 natural numbers? Please think step by step.\n","答：To find the sum of the first 100 natural numbers, we can use the formula for the sum of the first n natural numbers, which is n(n+1)/2. In this case, n=100, so the sum of the first 100 natural numbers is 100(100+1)/2 = 5050.\n","============================================================\n"]}],"source":["print(\"=\"*20 + \" #1 贪心解码 (Batch Generation) \" + \"=\"*20)\n","# 测试批量输入\n","query1 = [\n","    \"Give me a brief introduction to Shanghai Jiao Tong University.\", \n","    \"介绍一下上海交通大学。\", \n","    \"What is the capital of China?\"\n","]\n","\n","# 调用 generate，注意这里我们直接传入列表，测试【附加2】的 Batch 能力\n","responses_1 = generate(model, query1, max_new_tokens=256, do_sample=False)\n","\n","# 打印结果\n","if isinstance(responses_1, list):\n","    for i, (q, r) in enumerate(zip(query1, responses_1)):\n","        print(f\"[{i}] 问：{q}\")\n","        print(f\"    答：{r.strip()}\") # strip() 去除首尾可能的换行\n","        print(\"-\" * 50)\n","else:\n","    # 兼容性处理：如果没实现 Batch，返回的是单个字符串，但这会报错，所以上面加了 isinstance 判断\n","    print(\"Error: generate函数返回的不是列表，请检查是否正确实现了批量输入。\")\n","\n","\n","print(\"\\n\" + \"=\"*20 + \" #2 采样解码 (Sampling) \" + \"=\"*20)\n","query2 = \"Tell me a joke about computers.\"\n","# 测试单条输入，多次采样\n","for i in range(3): # 跑5次有点多，改为3次节省时间\n","    response = generate(model, query2, do_sample=True, temperature=0.7, top_p=0.9, top_k=50)\n","    print(f\"[{i+1}] 问：{query2}\")\n","    print(f\"    答：{response.strip()}\")\n","    print(\"-\" * 50)\n","\n","\n","print(\"\\n\" + \"=\"*20 + \" #3 【附加3】束搜索解码 (Beam Search) \" + \"=\"*20)\n","query3 = \"What is the sum of the first 100 natural numbers? Please think step by step.\"\n","# 测试 Beam Search\n","response_3 = generate(model, query3, num_beams=4, length_penalty=1.0)\n","print(f\"问：{query3}\")\n","print(f\"答：{response_3.strip()}\")\n","print(\"=\"*60)"]}],"metadata":{"kernelspec":{"display_name":"modularity","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.19"}},"nbformat":4,"nbformat_minor":4}
